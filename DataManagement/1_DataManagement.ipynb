{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop: Data Management with Python\n",
    "\n",
    "## Prerequisites\n",
    "### Installation:\n",
    "Minimum requirement is Python3.\n",
    "\n",
    "I personally do prefer the installation of the Anaconda Distribution. \n",
    "https://www.anaconda.com/products/individual\n",
    "\n",
    "Basically all packages you need to get started are included, as well as jupyter and spider editor. Also when installing new packages with conda\n",
    "```python\n",
    "conda install PACKAGE_NAME\n",
    "```\n",
    "the compatibility is taken care of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case a package is missing and you recognise when you want to import it, \n",
    "# you can do the installation also from within the notebook\n",
    "!pip install pandas\n",
    "#!conda install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For installation on Windows the command might need to look like:\n",
    "!{sys.executable} -m pip install pandas\n",
    "!{sys.executable} -m pip install xl..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages:\n",
    "For the data management tutorial we need packages for importing and exporting the data and for data wrangling. The most common package, which covers both is the **pandas module**. This\n",
    "Python open source library provides high-performance, easy-to-use data structures and data analysis tools.\n",
    "Another tool we need is the **numpy module**. This\n",
    "Python open source library makes handling of vectors, matrices and big multidimensional arrays easy.\n",
    "\n",
    "So after the installation of numpy and pandas package, you have to import them at the beginning of your python script or notebook to use their functionalities\n",
    "\n",
    "Good Practice tip: import as \"np\" and \"pd\" because it's faster to write \"pd\" than \"pandas\"\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "```\n",
    "\n",
    "### Data:\n",
    "At the very beginning we will look at a minimal Excel example Basics.xlsx to get to know some basic operations.\n",
    "\n",
    "Later on we will take a look at two examples of experimental data:\n",
    "- Tensile Test\n",
    "- Bubble Column\n",
    "\n",
    "### Plan B:\n",
    "If something went wrong with the installation or you cannot access the datafiles, you can view this notebook at nbviewer (passive) or at binder (interactive). Also you can view it directly on github.\n",
    "\n",
    "Links:\n",
    "- binder https://mybinder.org/v2/gh/astridwalle/python_jupyter_basics/HEAD?filepath=DataManagement%2F1_DataManagement.ipynb\n",
    "- nbviewer https://nbviewer.jupyter.org/github/astridwalle/python_jupyter_basics/blob/main/DataManagement/1_DataManagement.ipynb\n",
    "- github https://github.com/astridwalle/python_jupyter_basics\n",
    "\n",
    "## Now let's get started! What is this about? Why Python?\n",
    "### Separation of Data and Analysis!\n",
    "For example you have performed experimental analyses and now you have loads of raw data:\n",
    "\n",
    "There is actually no need to copy this data for analyzing and to store different analysis versions...\n",
    "\n",
    "Well-known szenarios are:\n",
    "\n",
    "- Multiple Excel files floating around on share drives\n",
    "- Lots of versions and local copies\n",
    "- Hard or even impossible to keep overview and trace changes\n",
    "- Changing the underlying data (add columns, change format, ...) can break everything\n",
    "\n",
    "\n",
    "### Typical Usecases for which Python has advantages over Excel, Matlab, ...\n",
    "For large datasets Excel becomes unhandy and slow. This is no problem for python due to the paradigm of separating the data from the analysis. If you want to try and store multiple approaches and analyses, this becomes a problem in Excel due to large file copies.\n",
    "With python your different analysis files stay small (just textfiles) and can also be version controlled (e.g. with git), which is great for collaboration.\n",
    "You don't end up with ..._final ..._final_final_\n",
    "\n",
    "With Python:\n",
    "- Import data and with that\n",
    "- Decouple Business Logic, Computations, Visualisation from the data\n",
    "- Access databases, xlsx tabels and more sources without changing them.\n",
    "- Export your analysis afterwards to every format you like (database, xlsx, ...)\n",
    "\n",
    "\n",
    "### Huge community of users and contributors\n",
    "You can google everything!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter Basics to get started:\n",
    "Check out the following short-cut and menu bar functionalities: \n",
    "- Markdown vs. Code\n",
    "- Coding Environment\n",
    "- auto-complete ```tab```\n",
    "- Running a cell: ```Run``` or ```Shift+Enter``` \n",
    "- ? in front of function call to get some help\n",
    "\n",
    "## Important for this course:\n",
    "The intention is to have an interactive working document, so\n",
    "- Add as many code cells as you like!\n",
    "- Try evrything!\n",
    "- Don't be shy, you cannot break anything!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example files for this class\n",
    "Basic sample data: \n",
    "```\n",
    "Data/Basics.xlsx\n",
    "```\n",
    "\n",
    "Real data from testing:\n",
    "- Tensile Testing\n",
    "```\n",
    "Data/TensileTest/AIMg3.txt Data/TensileTest/HDPE.txt Data/TensileTest/Staht.txt\n",
    "```\n",
    "- Bubble colum\n",
    "```\n",
    "Data/BubbleColumn/Test_01.xlsx Data/BubbleColumn/Test_02.xlsx Data/BubbleColumn/Test_03.xlsx \n",
    "```\n",
    "\n",
    "## Importing basic libraries\n",
    "\n",
    "**pandas module**\n",
    "\n",
    "Python open source library providing high-performance, easy-to-use data structures and data analysis tools. Built-in great capabilities for data import and export! (csv, xlsx, ...)\n",
    "\n",
    "Good Practice tip: import as \"np\" and \"pd\" because it's faster to call \"pd\" than \"pandas\"\n",
    "\n",
    "**numpy module** (We don't need it today)\n",
    "\n",
    "Python open source library for easy handling of vectors, matrices, big multidimensional arrays and mathematical operations\n",
    "\n",
    "## General notes\n",
    "### Modules\n",
    "\n",
    "You can either import a complete module, or just single classes or functions of a module.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "pd.read_csv(\"file.csv\")\n",
    "```\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(x,y)\n",
    "```\n",
    "\n",
    "## Classes and functions\n",
    "Python is a multi-paradigm programming language. You can do object oriented programming, but also structured, sequential programming. You can create objects with classes or just use functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's go!\n",
    "At first just one statement in a code cell to get a nice output. (This enables the print of multiple variables per coding cell.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make all print statements in a cell appear in output\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now it is your turn!\n",
    "1. Import the modules\n",
    "2. The functions of these modules are then called with pd.XXX\n",
    "3. Check which functions are available and how to use them\n",
    "4. Try pd. tab/autocomplete to see all available options\n",
    "5. Try inserting a ? in front of the function name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Press ```+``` and get started :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import module\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our first Testcases - Basics.xlsx / Tensile Testing\n",
    "If you have multiple tables, e.g. from different test runs, which you want to combine to one datasource for analyzing, that is pretty easy.\n",
    "\n",
    "We will do this for:\n",
    "1. One xlsx file with multiple sheets as raw data\n",
    "2. Multiple txt files as raw data\n",
    "\n",
    "Stitching together rowwise or columnwise is easy.\n",
    "\n",
    "### Exploratory Data Analysis\n",
    "We always start with an exploratory data analysis to ensure we know how our data looks like.\n",
    "\n",
    "### Our main datastructure: Pandas DataFrame\n",
    "For data analysis the most common module is pandas with the main tabular data structure DataFrame, which enables for endless operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start with reading in an Excel File.\n",
    "# To check the usage of the function we can put the questionmark in front of the function.\n",
    "# \n",
    "#?pd.ExcelFile\n",
    "#??pd.ExcelFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Read xlsx file - Variant 1 --> reads all the sheets in the file\n",
    "xls1=pd.ExcelFile(\"../Data/Basics.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xls1.sheet_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Read xlsx file - Variant 2 --> reads just the first sheet!\n",
    "xls2=pd.read_excel(\"../Data/Basics.xlsx\")\n",
    "xls2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use Variant 1 --> Parse the sheets into dataframes \n",
    "df_test1=xls1.parse(\"Test1\")\n",
    "df_test2=xls1.parse(\"Test2\")\n",
    "df_test3=xls1.parse(\"Test3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the results of the parsing on your own!\n",
    "# Always use autocompletion!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show results: have a look at the resulting dataframes\n",
    "# --> check the additional index column, which was added automatically\n",
    "df_test1\n",
    "df_test2\n",
    "df_test3\n",
    "# show no of columns and rows with df.shape\n",
    "df_test1.shape\n",
    "df_test2.shape\n",
    "df_test3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pd.concat - join dataframes\n",
    "\n",
    "**Task**\n",
    "\n",
    "At first we want to join the two dataframes df_test1 and df_test2 into one big DataFrame, which has \n",
    "1. all columns\n",
    "2. all rows\n",
    "Let's find out the caveats here...\n",
    "\n",
    "**hints**\n",
    "- have a look again at the dataframes:\n",
    "    - just type the name of the df and shift+enter\n",
    "    - have a look at number of rows and columns using df.shape\n",
    "- use **?** to find out how to use pd.concat\n",
    "- ask google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. all columns\n",
    "df_all_cols=pd.concat([df_test1,df_test2],axis=1,ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# show resulting dataframe\n",
    "df_all_rows=pd.concat([df_test1,df_test2],axis=0,ignore_index=True)\n",
    "df_all_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your turn: try around with\n",
    "- ?pd.concat\n",
    "- Change the axis\n",
    "- Change ignore_index\n",
    "- Try other options with autocomplete..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine different testruns into one dataframe\n",
    "\n",
    "If you want to merge different \"cases\", present in different tables, e.g. each representing another test or another parameter varation. pd.concat can join them into one df, adding an additional index, a so called multiindex\n",
    "\n",
    "We will do this now with our test tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_all_Params=pd.concat([df_Param1,df_Param2,df_Param3],axis=0,ignore_index=True)\n",
    "df_all_Params=pd.concat([df_test1,df_test2],keys=[\"Test1\",\"Test2\"],axis=0,names=[\"Param\",\"Row_Index\"],ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_Params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your turn: try out join / merge: \n",
    "Join and merge are also extremely interesting functionalities for dataframes. --> check it out now or later with\n",
    "```\n",
    "df_test1.join()\n",
    "df_test1.merge()\n",
    "```\n",
    "Ask google or add **?** to find out more. Use autocompletion!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's have a look at some real testing data\n",
    "```\n",
    "Data/TensileTest/AIMg3.txt\n",
    "```\n",
    "```\n",
    "Data/TensileTest/HDPE.txt\n",
    "```\n",
    "```\n",
    "Data/TensileTest/Stahl.tx`\n",
    "```\n",
    "\n",
    "### Your turn:\n",
    "\n",
    "- Read in all 3 files with the function pd.read_csv()\n",
    "- What are the caveats here?\n",
    "- Hint: read_csv can detect commas and whitespaces automatically, but here are tabs..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Read in multiple txt files\n",
    "# Try autocomplete- for everything! Paths, Variable names, functions, options...\n",
    "df_AlMg3=pd.read_csv(\"../Data/TensileTest/AlMg3.txt\", sep=\"\\t\", skiprows=0, header=None)\n",
    "\n",
    "# See the whole dataframe \n",
    "df_AlMg3\n",
    "\n",
    "# See the beginning or the end\n",
    "df_AlMg3.head(15)\n",
    "df_AlMg3.tail(10)\n",
    "# See a slice of the df\n",
    "df_AlMg3[5:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This looks a bit messy, so let's do some data wrangling\n",
    "# Split it up into metadata and actual data\n",
    "df_AlMg3_meta=df_AlMg3[1:9]\n",
    "df_AlMg3_data=df_AlMg3[12::]\n",
    "df_AlMg3_meta\n",
    "df_AlMg3_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And some more wrangling\n",
    "# Set menaningful columnnames\n",
    "df_AlMg3_meta.columns=[\"key\",\"value\",\"unit\"]\n",
    "df_AlMg3_data.columns=[\"Standardweg [mm]\",\"Standardkraft [N]\",\"None\"]\n",
    "df_AlMg3_meta\n",
    "df_AlMg3_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and at last we will cut the last column from the data, as this is empyt anyway:\n",
    "df_AlMg3_data.drop(columns=[\"None\"]).reset_index()\n",
    "# ATTENTION! Make sure that the action was really APPLIED and not just executed!\n",
    "df_AlMg3_data\n",
    "# so either you write the df new, or set the inplace option or copy\n",
    "# df_AlMg3=df_AlMg3_data.drop(columns=[\"None\"]).reset_index()\n",
    "df_AlMg3_data=df_AlMg3_data.drop(columns=[\"None\"])\n",
    "df_AlMg3_data.reset_index(inplace=True)\n",
    "# And we drop the empty rows of the metadata df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_AlMg3_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now that the data looks good, we can apply the transforming steps to the other datasets as well:\n",
    "### Option 1. Read one by one\n",
    "### Option 2. Read files in a loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls -al ../Data/TensileTest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# we can assign dynamic variable names with the loop index, but this is a bad idea. It is better \n",
    "# to write it into a so called dictionary. Later on we can access it by the name\n",
    "tests={}\n",
    "#for i in [\"HDPE\",\"Stahl\"]:\n",
    "for i in os.listdir(\"../Data/TensileTest/\"):\n",
    "    i=i.strip(\".txt\")\n",
    "    df=pd.read_csv(\"../Data/TensileTest/\"+i+\".txt\", sep=\"\\t\", skiprows=0, header=None)\n",
    "    df_meta=df[1:9]\n",
    "    df_data=df[12::]\n",
    "    df_meta.columns=[\"key\",\"value\",\"unit\"]\n",
    "    df_data.columns=[\"Standardweg [mm]\",\"Standardkraft [N]\",\"None\"]\n",
    "    df_data=df_data.drop(columns=[\"None\"])\n",
    "    df_data.reset_index(inplace=True)\n",
    "    key_data=i+\"_data\"\n",
    "    value_data=df_data\n",
    "    key_meta=i+\"_meta\"\n",
    "    value_meta=df_meta\n",
    "    tests[key_data]=value_data\n",
    "    tests[key_meta]=value_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we check the keys of the dictionary\n",
    "tests[\"AlMg3_meta\"][\"key\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now we combine all the testing data into one dataframe, using just the data values of the dictionary.\n",
    "df_tensile_tests=pd.concat([tests[\"AlMg3_data\"],tests[\"Stahl_meta\"]],keys=[\"AlMg3\",\"Stahl\"],axis=0,\n",
    "                  names=[\"Material\",\"Row_Index\"],ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the result\n",
    "df_tensile_tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Data Exploration with large datafiles...\n",
    "We always start by looking at the data. Get to know your data\n",
    "\n",
    "**Does the data look ok?**\n",
    "\n",
    "**Does it look as we expected it to be?**\n",
    "\n",
    "In the following we will list a few commands, which are helpful for data exploration!\n",
    "\n",
    "For this example we will use some data from the bubblecolumn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bub=pd.read_excel(\"../Data/BubbleColumn/Test_01.xlsx\")\n",
    "df_bub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column names to not look nice. Let's check the original fiel and adapt our function call.\n",
    "\n",
    "We need to give some extra options for reading in the files. --> Check the read_excel function with ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?pd.read_excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we give the first 2 lines as header --> then a multiindex is created.\n",
    "df_bub=pd.read_excel(\"../Data/BubbleColumn/Test_01.xlsx\",header=[0,1])\n",
    "df_bub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we will do some data exploration\n",
    "- shape\n",
    "- describe\n",
    "- slicing\n",
    "- accessing columns / rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bub.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bub.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the \"correct\" names for slicing and for looping\n",
    "df_bub.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access specific columns --> there are multiple ways to do so.\n",
    "df_bub.cam0\n",
    "df_bub.cam0[\"Waddel Disk Diameter\"]\n",
    "df_bub[\"cam0\"][\"Waddel Disk Diameter\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get an overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bub.shape\n",
    "df_bub.head(5)\n",
    "df_bub.tail(5)\n",
    "df_bub.index\n",
    "len(df_bub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 row\n",
    "df_bub[4:5]\n",
    "\n",
    "#multiple rows\n",
    "df_bub[10:20]\n",
    "\n",
    "#ignore last 100 rows\n",
    "df_bub[:-100]\n",
    "\n",
    "# 1 column\n",
    "df_bub[\"cam1\"][\"Bounding \\nleft\"]\n",
    "\n",
    "# multiple columns\n",
    "df_bub[df_bub.columns.values[:3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bub[df_bub[\"cam1\"][\"Bounding \\nleft\"]>32]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your task: \n",
    "Try filtering options with Boolean Algebra:\n",
    "- < / >\n",
    "- == / !=\n",
    "- multiple options combined with ```&```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some more filtering - groupby\n",
    "\n",
    "Filter data by categorical values\n",
    "\n",
    "Applies if you want to get single dataframes for specific groups.\n",
    "\n",
    "Example: RKI Covid Case Data - 1 row per day per Landkreis. To get all rows only for one Landkreis, you can use groupby."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also read the csv directly from url!\n",
    "df_rki=pd.read_csv(\"https://www.arcgis.com/sharing/rest/content/items/f10774f1c63e40168479a1feb6c7ca74/data\")\n",
    "df_rki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping works great to get separate DataFrames for different categories.\n",
    "df_grouped=df.groupby(\"Landkreis\")\n",
    "\n",
    "for name, dataframe in df_grouped:\n",
    "    print(name, len(dataframe))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Min / Max / Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bub[\"cam1\"][\"Bounding \\nleft\"].min()\n",
    "df_bub[\"cam1\"][\"Bounding \\nleft\"].max()\n",
    "df_bub[\"cam1\"][\"Bounding \\nleft\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes only sense for categorical values...\n",
    "df_bub[\"cam1\"][\"Bounding \\nleft\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rki[\"Bundesland\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get specific rows, columns, elements\n",
    "By names (loc),  indices (iloc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loc - gets you data by column and row name\n",
    "# get one specific element by column_name and row_index\n",
    "df_bub.loc[6,(\"cam1\",\"Bounding \\nleft\")]\n",
    "\n",
    "# get numerical index of column:\n",
    "idx=df_bub.columns.get_loc((\"cam1\",\"Bounding \\nleft\"))\n",
    "idx\n",
    "\n",
    "# iloc - gets you data by index\n",
    "# get one specific element by column index and row index\n",
    "df_bub.iloc[6,idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And what do we now do with that? More ideas... Try it!\n",
    "### Add other tests and combine to one big dataframe\n",
    "### Add columns with postprocessed values\n",
    "### Plotting\n",
    "### ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize results\n",
    "\n",
    "For the bubblecolumn test we plot v_pins over time\n",
    "\n",
    "More about data visualization in the next session! \n",
    "\n",
    "### Hint: You can also plot only a portion of the original data and apply the filtering functions upfront."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bub.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import datetime\n",
    "\n",
    "x=df_bub[\"erg\",\"Zeit [ms]\"]\n",
    "\n",
    "y1=(df_bub[\"erg\",\"z_bild \"].shift(1)-df_bub[\"erg\",\"z_bild \"])/(df_bub[\"erg\",\"t_Bilder LabV\"].shift(1)-df_bub[\"erg\",\"t_Bilder LabV\"])\n",
    "y2=df_bub[\"erg\",\"z_bild \"]\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.scatter(x,y1)\n",
    "plt.ylim(0,0.5)\n",
    "plt.xlabel(\"Zeit [ms]\")\n",
    "plt.ylabel(\"v_pins\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export\n",
    "### to Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just a tiny example. Of course you can do all kinds of formatting etc...\n",
    "writer = pd.ExcelWriter(\"../Data/df_all_columns.xlsx\",engine='xlsxwriter',options={'remove_timezone': True}) \n",
    "df_all_cols.to_excel(writer,sheet_name=\"all cols\",startrow=1 , startcol=1, index=False)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export notebooks also as\n",
    "- pdf\n",
    "- latex\n",
    "- py\n",
    "\n",
    "--> check out in the menu: File --> Export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More python:\n",
    "\n",
    "Some useful functionalities:\n",
    "- zip() - combines multiple iterables into one data structure by grouping them together according to their index (0 pairs with 0, 1 with 1, etc)  \n",
    "- map() -  map iterates over an array and executes a function on each element. It's an elegant and concise way to loop through data   \n",
    "- filter() - filters through your array and returns all elements who pass your condition  \n",
    "- reduce() - you can perform cumulative tasks on the elements of your list, for example the sum of all elements or calculating the product of all entries (has to be imported from functools for python 3)   \n",
    "- lambdas - Lambdas are locally defined functions you can use without having to define them globally\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from functools import reduce\n",
    "\n",
    "arr = [1, 2, 3, 4]\n",
    "\n",
    "letters = ['A', 'B', 'C', 'D']\n",
    "\n",
    "def someFunction(arg1, arg2):\n",
    "    result = arg1 ** arg2\n",
    "    return(result)\n",
    "\n",
    "print(someFunction(2, 3))\n",
    "\n",
    "outputZip = list(zip(arr, letters))\n",
    "print(outputZip)\n",
    "\n",
    "outputMap = list(map(lambda x: x*2, arr))\n",
    "print(outputMap)\n",
    "\n",
    "outputFilter = list(filter(lambda x: x % 2 == 0, arr))\n",
    "print(outputFilter)\n",
    "\n",
    "outputReduce = reduce(lambda x, y: x + y, arr)\n",
    "print(outputReduce)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More hacks and best practices:\n",
    "## Command mode\n",
    "```esc``` and then navigate around with arrows\n",
    "\n",
    "\n",
    "## Shell commands\n",
    "```python\n",
    "!ls\n",
    "```\n",
    "## Use virtual environments for more complex projects\n",
    "One big disadavantage of python is it's volatility and dynamic. So lots of functions keep changing and packages are not compatible with each other, depending on the versions.\n",
    "```python\n",
    "python3 -m venv --system-site-packages NAME_ENV\n",
    "```\n",
    "## Use the virtual env with jupyter notebook:\n",
    "```python\n",
    "pip install --user ipykernel\n",
    "python -m ipykernel install --user --name=myenv\n",
    "source env/bin/activate\n",
    "```\n",
    "\n",
    "## Get a working environment\n",
    "requirements.txt\n",
    "pip freeze\n",
    "Besides \n",
    "## Reuse the same structure for your projects --> Cookiecutter templates\n",
    "The way from raw to processed data is well documented, comprehensible and repeatable. \n",
    "\n",
    "### Hacks:\n",
    "- https://www.dataquest.io/blog/jupyter-notebook-tips-tricks-shortcuts/\n",
    "- https://github.com/astridwalle/python_jupyter_basics/blob/main/JupyterHacks/Jupyter-Hacks.ipynb\n",
    "\n",
    "### Combination of tools:\n",
    "https://sehyoun.com/blog/20180904_using-matlab-with-jupyter-notebook.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check out markdown possibilities\n",
    "e.g. include pics and gif's easily\n",
    "\n",
    "![Alt Text](https://media.giphy.com/media/vFKqnCdLPNOKc/giphy.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out your variables of a specific type, here now we check all DataFrames in our notebook\n",
    "%who DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
