{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop: Data Management with Python\n",
    "\n",
    "## Prerequisites\n",
    "### Installation:\n",
    "Minimum requirement is Python3.\n",
    "\n",
    "I personally do prefer the installation of the Anaconda Distribution. \n",
    "https://www.anaconda.com/products/individual\n",
    "\n",
    "Basically all packages you need to get started are included, as well as jupyter and spider editor. Also when installing new packages with conda\n",
    "```python\n",
    "conda install PACKAGE_NAME\n",
    "```\n",
    "the compatibility is taken care of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case a package is missing and you recognise when you want to import it, \n",
    "# you can do the installation also from within the notebook\n",
    "#!pip install pandas\n",
    "#!conda install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages:\n",
    "For the data management tutorial we need packages for importing and exporting the data and for data wrangling. The most common package, which covers both is the **pandas module**. This\n",
    "Python open source library provides high-performance, easy-to-use data structures and data analysis tools.\n",
    "Another tool we need is the **numpy module**. This\n",
    "Python open source library makes handling of vectors, matrices and big multidimensional arrays easy.\n",
    "\n",
    "So after the installation of numpy and pandas package, you have to import them at the beginning of your python script or notebook to use their functionalities\n",
    "\n",
    "Good Practice tip: import as \"np\" and \"pd\" because it's faster to write \"pd\" than \"pandas\"\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "```\n",
    "\n",
    "### Data:\n",
    "At the very beginning we will look at a minimal Excel example Basics.xlsx to get to know some basic operations.\n",
    "\n",
    "Later on we will take a look at two examples from your working group:\n",
    "- Tensile Test\n",
    "- Bubble Column\n",
    "\n",
    "### Plan B:\n",
    "If something went wrong with the installation or you cannot access the datafiles, you can view this notebook at nbviewer (passive) or at binder (interactive). Also you can download it from github.\n",
    "\n",
    "Links:\n",
    "- binder\n",
    "- nbviewer\n",
    "- github\n",
    "\n",
    "## Now let's get started! What is this about? Why Python?\n",
    "### Separation of Data and Analysis!\n",
    "For example if you have run experiments and you have raw data\n",
    "no need to copy to store different analysis versions\n",
    "\n",
    "- Multiple Excel files floating around on share drives\n",
    "- Lots of versions and local copies\n",
    "- Hard or even impossible to keep overview and trace changes\n",
    "- Changing the underlying data (add columns, change format, ...) can break everything\n",
    "\n",
    "\n",
    "### Typical Usecases for which Python has advantages over Excel, Matlab, ...\n",
    "large datasets. Excel becomes unhandy and slow. No problem for python. Storing multiple approache becomes a problem. Large copies.\n",
    "With python your different analysis files stay small (just textfiles) and can also be versioncontrolled (e.g. git), which is great for collaboration.\n",
    "No ..._final ..._final_final_\n",
    "\n",
    "- Import data and with that:\n",
    "- Decouple Business Logic, Computations, Visualisation from the data\n",
    "- Accessing databases, xlsx tabels and more sources without changing them.\n",
    "\n",
    "\n",
    "### Huge community of users and contributors\n",
    "You can google everything!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter Basics to get started:\n",
    "- Markdown vs. Code\n",
    "- Coding Environment\n",
    "- auto-complete ```tab```\n",
    "- Running a cell: ```Run``` or ```Shift+Enter``` \n",
    "- ? in front of function call to get some help\n",
    "\n",
    "## Important for this course:\n",
    "- Add as many code cells as you like!\n",
    "- Try evrything!\n",
    "- Don't be shy, you cannot break anything!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example files for this class\n",
    "Basic sample data: \n",
    "```\n",
    "Data/Basics.xlsx\n",
    "```\n",
    "\n",
    "Real data from testing:\n",
    "- Tensile Testing\n",
    "```\n",
    "Data/Zugversuche/AIMg3.txt Data/Zugversuche/HDPE.txt Data/Zugversuche/Staht.txt\n",
    "Data/Zugversuche/Testing.xlsx\n",
    "```\n",
    "- Bubble colum`\n",
    "```\n",
    "```\n",
    "\n",
    "## Importing basic libraries\n",
    "\n",
    "**numpy module**\n",
    "\n",
    "Python open source library for easy handling of vectors, matrices and big multidimensional arrays\n",
    "\n",
    "**pandas module**\n",
    "\n",
    "Python open source library providing high-performance, easy-to-use data structures and data analysis tools. Built-in great capabilities for data import and export! (csv, xlsx, ...)\n",
    "\n",
    "Good Practice tip: import as \"np\" and \"pd\" because it's faster to call \"pd\" than \"pandas\"\n",
    "\n",
    "\n",
    "## General notes\n",
    "### Modules\n",
    "\n",
    "You can either import a complete module, or just single classes or functions of a module.\n",
    "\n",
    "```\n",
    "import pandas as pd\n",
    "\n",
    "pd.read_csv(\"file.csv\")\n",
    "```\n",
    "\n",
    "```\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(x,y)\n",
    "```\n",
    "\n",
    "## Classes and functions\n",
    "Python is a multi-paradigm programming language. You can do object oriented programming, but also structured, sequential programming. You can create objects with classes or just use functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's go!\n",
    "### At first one statement to get a nice output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make all print statements in a cell appear in output\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now it is your turn!\n",
    "1. Import the modules\n",
    "2. The functions of these modules are then called with pd.XXX\n",
    "3. Check which functions are available and how to use them\n",
    "4. Try pd. tab/autocomplete to see all available options\n",
    "5. Try inserting a ? in front of the function name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now press ```+``` and get started :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import module\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our first Testcases - Basics.xlsx / Tensile Testing\n",
    "If you have multiple tables, e.g. from different test runs, which you want to combine to one datasource for analyzing, that is pretty easy.\n",
    "\n",
    "Will will do the same for \n",
    "1. One xlsx file with multiple sheets as raw data\n",
    "2. Multiple txt files as raw data\n",
    "\n",
    "Stitching together rowwise or columnwise is really easy.\n",
    "\n",
    "Do you know what your data looks like?\n",
    "\n",
    "### Our main datastructure: Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "#?pd.ExcelFile\n",
    "#??pd.ExcelFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Read xlsx file - Variant 1\n",
    "xls1=pd.ExcelFile(\"../Data/Basics.xlsx\")\n",
    "xls1.sheet_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Read xlsx file - Variant 2 --> reads just the first sheet!\n",
    "xls2=pd.read_excel(\"../Data/Basics.xlsx\")\n",
    "xls2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the sheets into dataframes \n",
    "df_test1=xls1.parse(\"Test1\")\n",
    "df_test2=xls1.parse(\"Test2\")\n",
    "df_test3=xls1.parse(\"Test3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show results: have a look at the resulting dataframes\n",
    "# --> check the additional index column, which was added automatically\n",
    "df_test1\n",
    "df_test2\n",
    "df_test3\n",
    "# show no of columns and rows with df.shape\n",
    "df_test1.shape\n",
    "df_test2.shape\n",
    "df_test3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pd.concat - join dataframes\n",
    "\n",
    "**Task**\n",
    "\n",
    "At first we want to join the two dataframes df_test1 and df_test2 into one big DataFrame, which has \n",
    "1. all columns\n",
    "2. all rows\n",
    "Let's find out the caveats here...\n",
    "\n",
    "**hints**\n",
    "- have a look again at the dataframes:\n",
    "    - just type the name of the df and shift+enter\n",
    "    - have a look at number of rows and columns using df.shape\n",
    "- use **?** to find out how to use pd.concat\n",
    "- ask google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. all columns\n",
    "df_all_cols=pd.concat([df_test1,df_test2],axis=1,ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show resulting dataframe\n",
    "df_all_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your turn: try around with\n",
    "- ?pd.concat\n",
    "- Change the axis\n",
    "- Change ignore_index\n",
    "- Try other options with autocomplete..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine different testruns into one dataframe\n",
    "\n",
    "If you want to merge different \"cases\", present in different tables, e.g. each representing another test or another parameter varation. pd.concat can join them into one df, adding an additional index, a so called multiindex\n",
    "\n",
    "We will do this now with our test tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_all_Params=pd.concat([df_Param1,df_Param2,df_Param3],axis=0,ignore_index=True)\n",
    "df_all_Params=pd.concat([df_test1,df_test2],keys=[\"Test1\",\"Test2\"],axis=0,names=[\"Param\",\"Row_Index\"],ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_Params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your turn: try out join / merge: \n",
    "Join and merge are also extremely interesting functionalities for dataframes. --> check it out now or later with\n",
    "```\n",
    "df_test1.join()\n",
    "df_test1.merge()\n",
    "```\n",
    "Ask google or add **?** to find out more. Use autocompletion!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's have a look at some real testing data\n",
    "```\n",
    "Data/TensileTest/AIMg3.txt\n",
    "```\n",
    "```\n",
    "Data/TensileTest/HDPE.txt\n",
    "```\n",
    "```\n",
    "Data/TensileTest/Stahl.tx`\n",
    "```\n",
    "\n",
    "### Your turn:\n",
    "\n",
    "- Read in all 3 files with the function pd.read_csv()\n",
    "- What are the caveats here?\n",
    "- Hint: read_csv can detect commas and whitespaces automatically, but here are tabs..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Read in multiple txt files\n",
    "# Try autocomplete- for everything! Paths, Variable names, functions, options...\n",
    "df_AlMg3=pd.read_csv(\"../Data/TensileTest/AlMg3.txt\", sep=\"\\t\", skiprows=0, header=None)\n",
    "# See the whole dataframe \n",
    "df_AlMg3\n",
    "# See the beginning or the end\n",
    "df_AlMg3.head(15)\n",
    "df_AlMg3.tail(10)\n",
    "# See a slice of the df\n",
    "df_AlMg3[5:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This looks a bit messy, so let's do some data wrangling\n",
    "# Split it up into metadata and actual data\n",
    "df_AlMg3_meta=df_AlMg3[1:9]\n",
    "df_AlMg3_data=df_AlMg3[12::]\n",
    "df_AlMg3_meta\n",
    "df_AlMg3_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And some more wrangling\n",
    "# Set menaningful columnnames\n",
    "df_AlMg3_meta.columns=[\"key\",\"value\",\"unit\"]\n",
    "df_AlMg3_data.columns=[\"Standardweg [mm]\",\"Standardkraft [N]\",\"None\"]\n",
    "df_AlMg3_meta\n",
    "df_AlMg3_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and at last we will cut the last column from the data, as this is empyt anyway:\n",
    "df_AlMg3_data.drop(columns=[\"None\"]).reset_index()\n",
    "# ATTENTION! Make sure that the action was really APPLIED and not just executed!\n",
    "df_AlMg3_data\n",
    "# so either you write the df new, or set the inplace option or copy\n",
    "# df_AlMg3=df_AlMg3_data.drop(columns=[\"None\"]).reset_index()\n",
    "df_AlMg3_data=df_AlMg3_data.drop(columns=[\"None\"])\n",
    "df_AlMg3_data.reset_index(inplace=True)\n",
    "# And we drop the empty rows of the metadata df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now that the data looks good, we can apply the transforming steps to the other datasets as well:\n",
    "### Option 1. Read one by one\n",
    "### Option 2. Read files in a loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# we can assign dynamic variable names with the loop index, but this is a bad idea. It is better \n",
    "# to write it into a so called dictionary. Later on we can access it by the name\n",
    "tests={}\n",
    "#for i in [\"HDPE\",\"Stahl\"]:\n",
    "for i in os.listdir(\"../Data/TensileTest/\"):\n",
    "    i=i.strip(\".txt\")\n",
    "    df=pd.read_csv(\"../Data/TensileTest/\"+i+\".txt\", sep=\"\\t\", skiprows=0, header=None)\n",
    "    df_meta=df[1:9]\n",
    "    df_data=df[12::]\n",
    "    df_meta.columns=[\"key\",\"value\",\"unit\"]\n",
    "    df_data.columns=[\"Standardweg [mm]\",\"Standardkraft [N]\",\"None\"]\n",
    "    df_data=df_data.drop(columns=[\"None\"])\n",
    "    df_data.reset_index(inplace=True)\n",
    "    key_data=i+\"_data\"\n",
    "    value_data=df_data\n",
    "    key_meta=i+\"_meta\"\n",
    "    value_meta=df_meta\n",
    "    tests[key_data]=value_data\n",
    "    tests[key_meta]=value_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests[\"AlMg3_meta\"][\"key\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tensile_tests=pd.concat([tests[\"AlMg3_data\"],tests[\"Stahl_meta\"]],keys=[\"AlMg3\",\"Stahl\"],axis=0,\n",
    "                  names=[\"Material\",\"Row_Index\"],ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tensile_tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Data Exploration with large datafiles...\n",
    "We always start by looking at the data. Get to know your data\n",
    "\n",
    "**Does the data look ok? **\n",
    "\n",
    "**Does it look as we expected it to be?**\n",
    "\n",
    "In the following we will list a few commands, which are helpful for data exploration!\n",
    "\n",
    "For the data exploration we will use some data from the bubblecolumn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bub=pd.read_excel(\"../Data/BubbleColumn/Test_01.xlsx\")\n",
    "df_bub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to give some extra options for reading in the files. --> Check the function with ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?pd.read_excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we give the first 2 lines as header --> then a multiindex is created.\n",
    "df_bub=pd.read_excel(\"../Data/BubbleColumn/Test_01.xlsx\",header=[0,1])\n",
    "df_bub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we will do some data exploration\n",
    "- shape\n",
    "- describe\n",
    "- slicing\n",
    "- accessing columns / rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bub.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bub.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the \"correct\" names for slicing and for looping\n",
    "df_bub.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access specific columns --> there are multiple ways to do so.\n",
    "df_bub.cam0\n",
    "df_bub.cam0[\"Waddel Disk Diameter\"]\n",
    "df_bub[\"cam0\"][\"Waddel Disk Diameter\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get an overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bub.shape\n",
    "df_bub.head(5)\n",
    "df_bub.tail(5)\n",
    "df_bub.index\n",
    "len(df_bub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 row\n",
    "df_bub[4:5]\n",
    "\n",
    "#multiple rows\n",
    "df_bub[10:20]\n",
    "\n",
    "#ignore last 100 rows\n",
    "df_bub[:-100]\n",
    "\n",
    "# 1 column\n",
    "df_bub[\"cam1\"][\"Bounding \\nleft\"]\n",
    "\n",
    "# multiple columns\n",
    "df_bub[df_bub.columns.values[:3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bub[df_bub[\"cam1\"][\"Bounding \\nleft\"]>32]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your task: Try filtering options:\n",
    "- < / >\n",
    "- == / !=\n",
    "- multiple options combined with ```&```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some more filtering - groupby\n",
    "\n",
    "Filter data by categorical values\n",
    "\n",
    "Applies if you want to get single dataframes for specific groups.\n",
    "\n",
    "Example: RKI Covid Case Data - 1 row per day per Landkreis. To get all rows only for one Landkreis, you can use groupby."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also read the csv directly from url!\n",
    "df=pd.read_csv(\"https://www.arcgis.com/sharing/rest/content/items/f10774f1c63e40168479a1feb6c7ca74/data\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_grouped=df.groupby(\"Landkreis\")\n",
    "\n",
    "for name, dataframe in df_grouped:\n",
    "    print(name, len(dataframe))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Min / Max / Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bub[\"cam1\"][\"Bounding \\nleft\"].min()\n",
    "df_bub[\"cam1\"][\"Bounding \\nleft\"].max()\n",
    "df_bub[\"cam1\"][\"Bounding \\nleft\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes only sense for categorical values...\n",
    "df_bub[\"cam1\"][\"Bounding \\nleft\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get specific rows, columns, elements\n",
    "By names (loc),  indices (iloc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loc - gets you data by column and row name\n",
    "# get one specific element by column_name and row_index\n",
    "type(df_bub.loc[6,(\"cam1\",\"Bounding \\nleft\")])\n",
    "df_bub.loc[6,(\"cam1\",\"Bounding \\nleft\")]\n",
    "\n",
    "# get numerical index of column:\n",
    "idx=df_bub.columns.get_loc((\"cam1\",\"Bounding \\nleft\"))\n",
    "idx\n",
    "\n",
    "# iloc - gets you data by index\n",
    "# get one specific element by column index and row index\n",
    "df_bub.iloc[6,idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And what do we now do with that? More ideas... Try it!\n",
    "### Add other tests and combine to one big dataframe\n",
    "### Add columns with postprocessed values\n",
    "### Plotting\n",
    "### ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize results\n",
    "\n",
    "For the bubblecolumn test we plot v_pins over time\n",
    "\n",
    "More about data visualization in the next session! \n",
    "\n",
    "### Hint: You can also plot only a portion of the original data and apply the filtering functions upfront."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bub.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline not needed for newer versions of juypter\n",
    "import datetime\n",
    "\n",
    "x=df_bub[\"erg\",\"Zeit [ms]\"]\n",
    "\n",
    "y=(df_bub[\"erg\",\"z_bild \"].shift(1)-df_bub[\"erg\",\"z_bild \"])/(df_bub[\"erg\",\"t_Bilder LabV\"].shift(1)-df_bub[\"erg\",\"t_Bilder LabV\"])\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.scatter(x,y)\n",
    "plt.ylim(0,0.5)\n",
    "plt.xlabel(\"Zeit [ms]\")\n",
    "plt.ylabel(\"v_pins\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export\n",
    "### to Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just a tiny example. Of course you can do all kinds of formatting etc...\n",
    "writer = pd.ExcelWriter(\"../Data/df_all_columns.xlsx\",engine='xlsxwriter',options={'remove_timezone': True}) \n",
    "df_all_cols.to_excel(writer,sheet_name=\"all cols\",startrow=1 , startcol=1, index=False)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export notebooks also as\n",
    "- pdf\n",
    "- latex\n",
    "- py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More python:\n",
    "\n",
    "Some useful functionalities:\n",
    "- zip() - combines multiple iterables into one data structure by grouping them together according to their index (0 pairs with 0, 1 with 1, etc)  \n",
    "- map() -  map iterates over an array and executes a function on each element. It's an elegant and concise way to loop through data   \n",
    "- filter() - filters through your array and returns all elements who pass your condition  \n",
    "- reduce() - you can perform cumulative tasks on the elements of your list, for example the sum of all elements or calculating the product of all entries (has to be imported from functools for python 3)   \n",
    "- lambdas - Lambdas are locally defined functions you can use without having to define them globally\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from functools import reduce\n",
    "\n",
    "arr = [1, 2, 3, 4]\n",
    "\n",
    "letters = ['A', 'B', 'C', 'D']\n",
    "\n",
    "def someFunction(arg1, arg2):\n",
    "    result = arg1 ** arg2\n",
    "    return(result)\n",
    "\n",
    "print(someFunction(2, 3))\n",
    "\n",
    "outputZip = list(zip(arr, letters))\n",
    "print(outputZip)\n",
    "\n",
    "outputMap = list(map(lambda x: x*2, arr))\n",
    "print(outputMap)\n",
    "\n",
    "outputFilter = list(filter(lambda x: x % 2 == 0, arr))\n",
    "print(outputFilter)\n",
    "\n",
    "outputReduce = reduce(lambda x, y: x + y, arr)\n",
    "print(outputReduce)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More hacks and best practices:\n",
    "## Command mode\n",
    "```esc``` and then navigate around with arrows\n",
    "\n",
    "\n",
    "## Shell commands\n",
    "```python\n",
    "!ls\n",
    "```\n",
    "## Use virtual environments for more complex projects\n",
    "One big disadavantage of python is it's volatility and dynamic. So lots of functions keep changing and packages are not compatible with each other, depending on the versions.\n",
    "```python\n",
    "python3 -m venv --system-site-packages NAME_ENV\n",
    "```\n",
    "## Use the virtual env with jupyter notebook:\n",
    "```python\n",
    "pip install --user ipykernel\n",
    "python -m ipykernel install --user --name=myenv\n",
    "source env/bin/activate\n",
    "```\n",
    "\n",
    "## Get a working environment\n",
    "requirements.txt\n",
    "pip freeze\n",
    "Besides \n",
    "## Reuse the same structure for your projects --> Cookiecutter templates\n",
    "The way from raw to processed data is well documented, comprehensible and repeatable. \n",
    "\n",
    "### Hacks:\n",
    "- https://www.dataquest.io/blog/jupyter-notebook-tips-tricks-shortcuts/\n",
    "- https://github.com/astridwalle/python_jupyter_basics/blob/main/JupyterHacks/Jupyter-Hacks.ipynb\n",
    "\n",
    "### Combination of tools:\n",
    "https://sehyoun.com/blog/20180904_using-matlab-with-jupyter-notebook.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check out markdown possibilities\n",
    "e.g. easy include pics \n",
    "![Alt Text](https://media.giphy.com/media/vFKqnCdLPNOKc/giphy.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
